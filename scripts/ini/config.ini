[common]
multi_tune = False
max_runs = 10


[database]
# db: [mysql, postgresql]
db = postgresql
# Host IP Address
host = 
# Host SSH Port
ssh_port = 
# Host Port Number
port = 
# Database User Name
user = postgres
# Database Password
passwd = 
# db cnf file on clientDB host
cnf = /home/postgres/experiment_normandy.cnf


####### PostgreSQL related config
# path to pg_ctl on clientDB host
pg_ctl = /home/postgres/postgres/pg13/bin/pg_ctl
# path to PGDATA on clientDB host
pgdata = /home/postgres/postgres/pg13/data
# path to postgres on clientDB host
postgres = /home/postgres/postgres/pg13/bin/postgres

####### DB knob related
# knob config file
knob_config_file =  scripts/experiment/gen_knobs/postgres_new.json


####### Workload related
# Database Name
dbname = twitter
# workload name [sysbench, tpcc, workload_zoo,  oltpbench_twitter, oltpbench_ycsb]
workload = oltpbench_twitter
# oltpbenchmark config
oltpbench_config_xml = 
# thread_num
thread_num = 
# [`read`, `write`, `readwrite`]
workload_type = 
# workload warmup time
workload_warmup_time = 
# workload run time
workload_time = 

####### Remote tuning related
# whether tune remotely
remote_mode = True
# username on remote host
ssh_user = postgres
private_key = ~/.ssh/id_ed25519
####### Online tuning related
# whether not restart db
online_mode = False


[tune]
# task id
task_id = 
# performance_metric: [tps, lat, qps, cpu, IO, readIO, writeIO, virtualMem, physical]
# default maximization, '- 'minus means minimization
performance_metric = ['tps']
# set for multi-objective tuning
reference_point = [None, None]
#constraints: Non-positive constraint values (”<=0”) imply feasibility.
; constraints = ["100-tps", "readIO - 100"]
constraints = []
# maximum tuning iteration
max_runs = 100
# reaload data every iteration
reload_data = False

only_knob = False
only_range = False

############Knob Selection###############
# selector_type [shap, fanova, gini, ablation, lasso]
selector_type = shap
# initial_runs
initial_runs = 2
#tunalbe_knob_num
initial_tunable_knob_num = 30
#incremental: [none, increase, decrease]
incremental = none
incremental_every = 10
incremental_num = 2

############Optimizer###############
auto_optimizer = False
# tuning method [MBO, SMAC, TPE, DDPG, TurBO, GA]
optimize_method = SMAC
acq_type = eit

###TurBO####
# whether TurBO start from the scratch
tr_init = True

############Transfer###############
space_transfer = False
# transfer_framework :[none, workload_map, ... finetune]
transfer_framework = fine_grained
# dir of source data for mapping
data_repo = DBTune_history_experiment
